---
slug: "/ethics-guidelines-will-save-us"
abstract: "In this section we'll discuss how everyone seems to be writing up and signing on to AI ethics guidelines, and what role they can have in governing AI. We'll see how critics claim that ethics guidelines are just a way for companies to avoid regulation, and look at some alternatives."
metaDescription: "We discuss the boom in AI ethics guidelines, what role they have in governing AI, and how human rights can offer a better framework."
---

Just ten years ago, the prospect of finding a job as an artificial intelligence ethicist would have sounded more like something out of a sci-fi story than a real job. And yet, in early 2019, KPMG published a list of the [_Top 5 AI hires companies need to succeed in 2019_](https://info.kpmg.us/news-perspectives/technology-innovation/top-5-ai-hires-companies-need-to-succeed-in-2019.html) with AI ethicist at number 5 on the list.

Indeed, 2019 was quite the year for AI ethics as a phenomenon: on the one hand, it reached unexpected prominence in policy debates as the silver bullet to govern AI without stifling innovation; on the other, there was a backlash against ethics guidelines as a ruse to avoid regulation that coined the term [**AI-ethics-washing**](https://www.tagesspiegel.de/politik/eu-guidelines-ethics-washing-made-in-europe/24195496.html). To get some clarity on the role of ethics in the governance of AI, let’s begin by unpacking some of the key concepts.

First of all, what <rich-link text="kind of ethics">For a discussion of the different definitions of ethics that are relevant here, see [Thinking about 'ethics' in the ethics of AI](https://revistaidees.cat/en/thinking-about-ethics-in-the-ethics-of-ai/)</rich-link> are we talking about? This question could obviously occupy us for a long time, but we can restrict ourselves to a limited domain. We want to look at the question of ethics, and specifically ethics guidelines and principles, as a form of AI governance which is typically presented as an alternative to regulation. 

This means that we will not discuss what is known as <rich-link text="machine ethics">See, for example, [Machine ethics: The robot’s dilemma](https://www.nature.com/news/machine-ethics-the-robot-s-dilemma-1.17881), or this article for a critical view on the idea of machine ethics: [Critiquing the Reasons for Making Artificial Moral Agents - Aimee van Wynsberghe & Scott Robbins](https://link.springer.com/article/10.1007/s11948-018-0030-8)</rich-link>, i.e. the study of how to create machines that can perform ethical reasoning, also known as **artificial moral agents**. Some resources have been provided about this topic in the bibliography, however. We will also not discuss the related issues of ethics and superintelligence, such as the **control problem**. Some of these topics are discussed in another section (see: [Superintelligence is coming soon](/superintelligence-is-coming-soon)).

##The AI ethics boom
To understand the role of ethics guidelines in AI governance, it's instructive to begin with some numbers. A 2019 study by Anna Jobim et al. entitled, [_The global landscape of AI ethics guidelines_](https://www.nature.com/articles/s42256-019-0088-2), “identified **84 documents containing ethical principles or guidelines for AI**[...] with **88% having been released after 2016**.” The [inventory of AI ethics guidelines](https://inventory.algorithmwatch.org/) compiled by Algorithm Watch lists over 160, and is constantly being updated.

Another study of the AI ethics landscape is the [Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI](https://dash.harvard.edu/handle/1/42160420) from Harvard’s Berkman Klein Center. In this study, the authors analyse 36 sets of AI principles, and note a ‘convergence’ around 8 key themes shared by the various sets of principles:

- Privacy
- Accountability
- Safety and Security
- Transparency and Explainability
- Fairness and Non-discrimination
- Human Control of Technology
- Professional Responsibility
- Promotion of Human Values

Although they note that these principles could be seen to represent a **normative core**, they caution against drawing any overly optimistic from this apparent convergence. As they note:

> On its own, a set of principles is unlikely to be more than gently persuasive. Its impact is likely to depend on how it is embedded in a larger governance ecosystem, including for instance relevant policies (e.g. AI national plans), laws, regulations, but also professional practices and everyday routines.

Indeed, while we could see this apparent convergence as a sign that we’re moving towards some common ethical foundation, we need to acknowledge that it is far easier for companies and governments to sign up to relatively vague ethical principles than it is for them to change business practices or enforce restrictive laws.

If principles committing companies to fairness and non-discrimination had serious legal consequences, we’d surely have far more heated disagreement about precisely how to define these principles (there are, after all, [21 different definitions of fairness in machine learning](https://www.youtube.com/watch?reload=9&v=jIXIuYdnyyk)). Regarding companies committing to 'privacy' as a principle, one can think of the joke that while Facebook say that they “take our privacy seriously,” in reality they “take our privacy, seriously.”

On a serious note, concepts such as fairness in machine learning are not only contentious, but researchers have proven that [different definitions of fairness are in fact mutually exclusive](https://arxiv.org/pdf/1703.00056.pdf). This means that two companies can be committed to fairness in AI systems in conflicting ways.

In the idea that AI should “promote human values” we also see a problem: all values are, arguably, human values. The values driving the worst atrocities of human history were ‘human values,’ so it’s hard to take solace in such a commitment, especially if it comes from companies who are notorious for putting profit ahead of human and environmental welfare.

It certainly seems clear that there was an uptick in the production of ethics guidelines for AI beginning in 2016, but what caused this? It's hard to say with certainty, but one possibility is that the enthusiasm (and funding) for drafting ethical principles for AI coincided with the fear of governments introducing regulation for AI.

At this time, AI was emerging as the new buzzword (following the reign of 'Big Data'), but the public were beginning to hear more and more about scandals involving AI technology. In 2015, for example, controversy had erupted after Google's photos app [tagged images of a Black couple as 'gorillas](https://www.theguardian.com/technology/2015/jul/01/google-sorry-racist-auto-tag-photo-app).' 2016 also saw the release of Cathy O'Neil's seminal investigation of the negative societal impact of algorithmic systems, [_Weapons of Math Destruction_](https://www.theguardian.com/technology/2015/jul/01/google-sorry-racist-auto-tag-photo-app).

Indeed, this role of ethics in dodging regulation is precisely what Rodrigo Ochigame claimed in an inflamatory article about the role of Big Tech in promoting the 'new discipline' of AI ethics:

> To characterize the corporate agenda, it is helpful to distinguish between **three kinds of regulatory possibilities** for a given technology: (1) **no legal regulation at all**, leaving “ethical principles” and “responsible practices” as merely voluntary; (2) **moderate legal regulation** encouraging or requiring technical adjustments that do not conflict significantly with profits; or (3) **restrictive legal regulation curbing or banning** deployment of the technology. Unsurprisingly, the tech industry tends to support the first two and oppose the last. The corporate-sponsored discourse of “ethical AI” enables precisely this position.

This idea of ethics guidelines serving as means to dodge regulation is what gave rise to the accusation of **'ethics washing.'** 

This accusation was most [famously made by Thomas Metzinger](https://www.tagesspiegel.de/politik/eu-guidelines-ethics-washing-made-in-europe/24195496.html), a philosophy professor and member of the European Union's High Level Expert Group on AI, who accused the group of ethics washing because the '[Ethics Guidelines for Trustworthy AI](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai)' which they produced had been weakened due to industry dominance within the group.

##Ethics as a practice vs. ethics as a governance mechanism
It’s maybe helpful at this point to make a distinction between two levels of how we look at ‘AI ethics.’ On the one hand, we can look at things at the team level, where people developing AI systems engage in processes of ethical reflection to improve their products.

This could involve developers working with ethicists (whether as part of the team or as consultants), or using one of the variety of technical tools that have been developed to ensure technical fairness in AI systems (notable examples are [IBM’s Fairness 360](https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/) and Google’s [What-If tool](https://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html)). If such practices encourage ethical reflection and thereby improve products, it can't be a bad thing in itself.

On the other hand, we can look at the phenomenon of ethics at an institutional level where it's supposed to function as a governance mechanism. Many of the biggest tech companies are signatories to numerous sets of ethical guidelines, and yet routinely roll out AI products that cause harm. Can be any real impact if a company violates a principle from a set of ethical guidelines that they signed onto? Or are we just going to hear the tired response that they will "[work to do better](https://www.fastcompany.com/40547045/a-brief-history-of-mark-zuckerberg-apologizing-or-not-apologizing-for-stuff)."

Moreover, there are plenty of governments that have signed up to nice-sounding guidelines, such as the [OECD Principles on AI](https://www.oecd.org/going-digital/ai/principles/), and yet are actively investing in and rolling out obviously unethical AI systems such as live facial recognition in public spaces.

In the case of companies, there's likely no case in which having an ethicist on board would stop serious abuses, because we need to take account of the power dynamics in these situations. How much impact can an individual ethicist have, especially if, as many claim, the negative consequences of some of these technologies are not bugs, but features?

This is especially the case when the conclusion of an ‘ethical assessment’ would be that the product simply shouldn’t be developed. There is no way, for example, to make something like [Clearview AI’s facial recognition](https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html) product ‘ethical’; it simply shouldn't exist, and this is probably not going to be considered constructive feedback at a team meeting. 

Similarly, if an independent ethics board came to the conclusion that the business model of Facebook or Twitter requires fundamental changes to stop the platform from causing harm, is it likely that these platforms would (or could) follow the advice?

There is also a flawed assumption that we can have agreement on both the definition and selection of ethical principles, and on their application. A good example of this is the first of [Google’s AI principles](https://ai.google/principles/):

> As we consider potential development and uses of AI technologies, we will take into account a broad range of social and economic factors, and will proceed where we believe that the **overall likely benefits substantially exceed the foreseeable risks and downsides**.

The obvious questions to ask here are: **benefit for whom, and harm for whom**? If this really means 'benefits for Google and its shareholders' versus 'harm for people and communities who are subject to its technologies,' then it seems likely that the benefits will be given more weight than the harms. We should also ask whether those likely to be harmed are getting meaningful input into these moral calculations.

This problem of weighing up benefits against harms raises the question of whether **utilitarian** approaches to ethics (where the idea is to maximise benefit and/or minimize harm) are really the right way to approach the ethical challenges raised by AI systems. 

In contrast to such approaches which seek to achieve 'net benefit' by weighing up one person's harm against another's gain, an approach **grounded in human rights** begins from the starting point that **certain harms are simply unacceptable**, so let's take a look at how the human rights framework could address some of these problems.

##Human rights as an alternative framework
Many of the issues raised so far against AI ethics as a form of governance can arguably be better taken care of by applying a human rights lens to AI. One of the earliest applications of the human rights framework to the topic of AI was the 2018 [Toronto Declaration - Protecting the right to equality and non-discrimination in machine learning systems](https://www.torontodeclaration.org/). Since then there has been an ever increasing amount of work in this area, with academics, civil society organisations and international bodies all publishing <rich-link text="work on human rights and AI">See, for example, the Council of Europe's [work on AI](https://www.coe.int/en/web/artificial-intelligence), especially the report [Unboxing Artificial Intelligence: 10 steps to protect Human Rights](https://rm.coe.int/unboxing-artificial-intelligence-10-steps-to-protect-human-rights-reco/1680946e64). There are also a number of NGOs working to ensure that AI development protects human rights, such as [Access Now](https://www.accessnow.org/), who produced a report on [Human Rights in the Age of Artificial Intelligence](https://www.accessnow.org/human-rights-matter-in-the-ai-debate-lets-make-sure-ai-does-us-more-good-than-harm/), and [Amnesty International](https://www.amnesty.org/en/latest/research/2019/06/ethical-ai-principles-wont-solve-a-human-rights-crisis/). See the bibliography for more resources.</rich-link>.

Whereas voluntary ethics guidelines leave large scope for companies to interpret what different principles mean, the international human rights framework has established mechanisms for resolving such ambiguities, and enforcing compliance, even if that hasn't always been without issues. As the authors of the [Principled Artificial Intelligence](https://dash.harvard.edu/handle/1/42160420) study noted:

> Existing mechanisms for the interpretation and protection of human rights may well provide useful input as principles documents are brought to bear on individuals cases and decisions, which will require precise adjudication of standards like “privacy” and “fairness,” as well as solutions for complex situations in which separate principles within a single document are in tension with one another

Given that many harmful uses of AI spring from good intentions, it’s also important to ask the kind of questions that the human rights framework motivates us to ask. This can mean asking **what harms are unacceptable** rather than whether the benefits outweigh the costs. This comes back to the more [deontological foundations of human rights](https://ethics.org.au/ethics-explainer-deontology/) in contrast to the utilitarianism that tends to dominate industry ethics principles. A human rights framework offers a greater chance of finding agreement about red lines, rather than aspirations, which is arguably more the advantage of ethics.

To circle back to our earlier distinction, we could say that ethics has an undeniable role improving how development teams think about certain issues, but lacks the ['teeth' to be effective as a form of governance](https://www.article19.org/resources/governance-with-teeth-how-human-rights-can-strengthen-fat-and-ethics-initiatives-on-artificial-intelligence/), which a human rights framework can do much better.

As a recent example of how a human rights framework can provide the red lines we need, the [UN Special Rapporteur on Contemporary Forms of Racism](https://www.ohchr.org/EN/NewsEvents/Pages/DisplayNews.aspx?NewsID=26101), E. Tendayi Achiume, has called for an "immediate moratorium on the sale, transfer and use of surveillance technology," and has further stated that in certain cases "it will be necessary to impose outright bans on technology that cannot meet the standards enshrined in international human rights legal frameworks prohibiting racial discrimination."

The human rights framework is not beyond criticism, of course. As Achiume has noted elsewhere, there has been a "general marginality of racial equality within the global human rights agenda" (see her piece, [Putting racial equality onto the global human rights agenda](https://www.ohchr.org/Documents/HRBodies/HRCouncil/AdvisoryCom/EliminationRacism/Achiume_Dec2018_Article_in_Sur.pdf)). In the domain of AI governance, Pak-Hang Wong has noted the serious challenged posed to the framework by respect for cultural pluralism (see [Cultural Differences as Excuses?](https://link.springer.com/content/pdf/10.1007/s13347-020-00413-8.pdf)). Nevertheless, both authors are positive that the human rights framework can rise to these challenges and better integrate both racial equality and a respect for cultural pluralism.

Let's take a closer look at how a human rights or civil rights lens (and other non-utilitarian frameworks) can help us see when tweaks to an algorithm are not enough and when we simply need to ban certain technologies.

##The case for a ban: when standards and guidelines aren't enough
It is important to note that a number of critics have pointed to the dangers of focusing on making harmful technologies 'more ethical.' Engaging with technologies such as facial recognition in order to improve them can have the consequence of legitimising them as a solution, whereas we should arguably be rejecting them wholesale.

As Ali Breland notes in his essay [_Woke AI won’t save us_](https://logicmag.io/bodies/woke-ai-wont-save-us/), “[t]he problem with the “woke AI” pushed by companies like IBM is that it asks us to see criminal justice in the same way that companies like Aetna want us to see healthcare: something that basically works fine, but which could use a few technological tweaks.”

In opposition to this perspective that sees AI development as merely needing some ethical guidance and improvement, we've seen an increase in **calls for certain systems to be prohibited**. Numerous activists and organisations are now calling for facial recognition technologies to be halted or even banned outright.

Coming from a human rights framework, a network of civil and human rights organisations from across Europe (EDRi), have called for a [ban on facial recognition technologies that enable mass surveillance](https://edri.org/blog-ban-biometric-mass-surveillance/). As they point out, such systems violate human rights in such an egregious manner that they simply have to be banned.

In the United States, calls for banning facial recognition have also increased in 2020, largely driven by the Black Lives Matter protests. As Malkia Devich-Cyril explains in this article, [Defund Facial Recognition](https://www.theatlantic.com/technology/archive/2020/07/defund-facial-recognition/613771/):

> [in] an era when policing and private interests have become extraordinarily powerful — with those interests also intertwined with infrastructure — short-term moratoriums, piecemeal reforms, and technical improvements on the software won’t defend Black lives or protect human rights [...] facial recognition and other forms of biometric policing don’t need more oversight, or to be reformed or improved. Facial recognition, like American policing as we know it, must go.

When it comes to AI technologies that undermine our rights, we need more than vague ethical principles and technical fairness tools. 

We cannot rely on the goodwill of companies to protect us from abuses caused by technologies which they have a vested interest in developing and deploying. Indeed, rather than companies proactively taking steps to prohibit certain technologies, much of the momentum so far has come from grassroots initiatives such as the [growing tech worker movement](https://logicmag.io/the-making-of-the-tech-worker-movement/full-text/).

Ultimately, ethics guidelines, no matter how well intentioned, will not suffice. We need governments and international insitutions to step up and draw red lines so that certain applications of AI - from biometric surveillance to predictive policing - are stopped in their tracks, and appropriate safeguards and accountability mechanisms are instituted for others.
